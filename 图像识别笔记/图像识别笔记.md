maxpooling主要有两大作用

1. invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)
2. 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力。
----------------------------------------------------------------------------------------------------
dropout用来减少过拟合。
----------------------------------------------------------------------------------------------------
Dense layer是用来做类别预测的。

----------------------------------------------------------------------------------------------------
flatten()函数用法

flatten是numpy.ndarray.flatten的一个函数，即返回一个一维数组。

flatten只能适用于numpy对象，即array或者mat，普通的list列表不适用！。

a.flatten()：a是个数组，a.flatten()就是把a降到一维，默认是按行的方向降 。

------------------------------------------------------------------------------------------------------


在机器学习尤其是深度学习中，softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。
首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是max。对于max我们都很熟悉，比如有两个变量a,b。如果a>b，则max为a，反之为b。用伪码简单描述一下就是 if a > b return a; else b。
另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
————————————————
版权声明：本文为CSDN博主「bitcarmanlee」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/bitcarmanlee/article/details/82320853
